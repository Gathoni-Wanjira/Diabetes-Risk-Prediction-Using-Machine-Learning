# ğŸ©º Diabetes Risk Prediction with Machine Learning

This project aims to predict the risk of diabetes using a combination of clinical and demographic data â€” think age, glucose levels, BMI, blood pressure, and more. The goal? Not just to build another ML model, but to empower early detection and proactive healthcare.

## ğŸ’¡ Problem Statement

Diabetes is a growing global health concern, and early detection is key to prevention and management. This project leverages supervised machine learning to identify individuals at high risk based on known health indicators.

## ğŸ§ª Dataset

The dataset contains anonymized patient records, including:
- Age
- Glucose levels
- BMI
- Blood Pressure
- Insulin
- Skin Thickness
- Pregnancies
- Outcome (binary: diabetic or not)

> ğŸ“¦ Data Source: Kaggle

## ğŸ”§ System Architecture

1. **Data Cleaning & Preprocessing**
   - Handled missing values and outliers
   - Scaled features using StandardScaler
   - Encoded categorical variables (if any)

2. **Exploratory Data Analysis**
   - Visualized class distributions
   - Correlation heatmaps & feature distributions
   - Feature importance ranking

3. **Feature Engineering**
   - Feature selection based on correlation & model importance
   - Removed irrelevant or low-impact features

4. **ML Model Training**
   - Random Forest
   - Neural Network
   - SVM (RBF)
   - Logistic Regression (as baseline)

5. **Model Evaluation**
   - Accuracy
   - ROC-AUC
   - Precision, Recall & F1-score

6. **Deployment Ready**
   - Final model serialized using `joblib`

### ğŸ“Š Model Performance Results

| **Algorithm**         | **Accuracy** | **Precision** | **Recall** | **F1-Score** | **AUC-ROC** |
|-----------------------|--------------|---------------|------------|--------------|-------------|
| ğŸŸ¢ Random Forest       | **92.3%**     | **89.7%**      | **85.4%**   | **87.5%**     | **0.94**     |
| Neural Network        | 91.8%        | 88.9%         | 84.6%      | 86.7%        | 0.93        |
| SVM (RBF Kernel)      | 90.5%        | 87.2%         | 82.1%      | 84.6%        | 0.91        |
| Logistic Regression   | 88.7%        | 85.3%         | 79.8%      | 82.4%        | 0.89        |


## ğŸŒŸ Key Takeaways

- XGBoost consistently outperformed baseline models
- Age, glucose levels, and BMI were the most predictive features
- Feature engineering & balanced class training made a huge difference
- Early intervention starts with better predictions ğŸ’™

## ğŸ“ Repository Structure

â”œâ”€â”€ data

â”œâ”€â”€ notebooks

â”œâ”€â”€ src

â”‚ â”œâ”€â”€ preprocessing.py

â”‚ â”œâ”€â”€ train_model.py

â”‚ â””â”€â”€ evaluate_model.py

â”œâ”€â”€ model

â”‚ â””â”€â”€ final_model.joblib

â”œâ”€â”€ README.md

â””â”€â”€ requirements.txt


## ğŸ› ï¸ Tech Stack

- Python
- Pandas, NumPy
- Scikit-learn
- XGBoost
- Matplotlib & Seaborn
- Jupyter Notebooks

## ğŸš€ Future Improvements

- Add FastAPI or Flask backend for real-time predictions
- Expand dataset with additional features (e.g., lifestyle, genetic factors)
- Integrate SHAP or LIME for model explainability

## ğŸ¤ Contributions

Pull requests are welcome! For major changes, please open an issue first to discuss what you'd like to change.

---

*Built with care by [Gathoni Wanjira](https://www.linkedin.com/in/gathoni-wanjira/).*


